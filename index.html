<!DOCTYPE html lang="en">
<html lang="en">
<head>
    <title>AR</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
</head>

<body>
    
  <img id='bitmap' src="/verk5-vidmot/firecard.png" style="display: none;">
    <script src="https://unpkg.com/three@0.126.0/build/three.js"></script>
    <script src="https://unpkg.com/three@0.126.0/examples/js/loaders/GLTFLoader.js"></script>
    <div>
        <button id='webcamButton' style="width: 100%; padding: 20px; background-color: red;">ENABLE WEBCAM</button>
        <div style='position: relative;'>
            <video id='webcam' autoplay style='transform: scaleX(-1);'></video>
            <canvas id='output_canvas' style='position: absolute; left: 0px; top: 0px; transform: scaleX(-1);'></canvas>
            <h1 id='gesture_output'></h1>
            <h2>X <span id='x_output'></span></h2>
            <h2>Y <span id='y_output'></span></h2>
        </div>
    </div>
  <script type="module">

    import { GestureRecognizer, FilesetResolver } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3';
      let gestureRecognizer;
      let runningMode = 'IMAGE';
      let enableWebcamButton;
      let webcamRunning = false;
      const videoWidth = window.screen.width;

      const createGestureRecognizer = async () => { 
        const vision = await FilesetResolver.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm');
          gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
            baseOptions: {
              modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task',
                delegate: 'GPU'
            },
            runningMode: runningMode,
          });
        };

      createGestureRecognizer();

      const video = document.getElementById('webcam');
      const gestureOutput = document.getElementById('gesture_output');
      const xOutput = document.getElementById('x_output');
      const yOutput = document.getElementById('y_output');
      let lastVideoTime = -1;
      let results = undefined;
      let model;
      let isShot = false;

      let shootDirection = new THREE.Vector3();
      let shootSpeed = 0.005;
      let velocity = shootDirection.multiplyScalar(shootSpeed);

      const lateralMovementFactor = 0.5;
      const verticalMovementFactor = 0.5;

      let pos;
      let quat;

      enableWebcamButton = document.getElementById('webcamButton');

      enableWebcamButton.addEventListener('click', () => {
        enableCam();
        AR();
      });

      function enableCam(event) {
        if (!gestureRecognizer) {
            alert('Please wait for gestureRecognizer to load');
            return;
        }
        if (webcamRunning === true) {
            webcamRunning = false;
            return;
        }
        webcamRunning = true;

        const constraints = {
            video: {
              facingMode: "user",
              width: { ideal: 640 },
              height: { ideal: 480 }
            }
        };

        navigator.mediaDevices.getUserMedia(constraints)
        .then(function (stream) {
            video.srcObject = stream;
            video.addEventListener('loadeddata', predictWebcam);
        })
        .catch(function (err) {
            console.log("An error occurred: " + err);
            alert("An error occurred: " + err);
        });
      }

      async function predictWebcam() {
          const webcamElement = document.getElementById('webcam');
          if (runningMode === 'IMAGE') {
              runningMode = 'VIDEO';
              await gestureRecognizer.setOptions({ runningMode: 'VIDEO' });
          }

          let nowInMs = Date.now();

          if (video.currentTime !== lastVideoTime) {
              lastVideoTime = video.currentTime;
              results = gestureRecognizer.recognizeForVideo(video, nowInMs);
          }

          if (results.gestures.length > 0) {
              gestureOutput.style.display = 'block';
              gestureOutput.style.width = videoWidth;
              gestureOutput.innerText = results.gestures[0][0].categoryName;
              
              parseFloat(xOutput.innerText = results.landmarks[0][0].x.toFixed(2));
              parseFloat(yOutput.innerText = results.landmarks[0][0].y.toFixed(2));
              console.log(gestureOutput.innerText)
          }
          else {
              gestureOutput.style.display = 'none';
          }

          if (webcamRunning === true) {
              window.requestAnimationFrame(predictWebcam);
          }
      }
    
      const img  = document.getElementById('bitmap');
      let imgBitmap = null
      createImageBitmap(img).then(x=>{imgBitmap = x});

      let camera, scene, renderer, xrRefSpace, gl;

      scene = new THREE.Scene();

      const loader = new THREE.GLTFLoader();

      loader.load("https://bjornthor21.github.io/verk5-vidmot/fireball.glb", function(gltf) {
        model = gltf.scene;
        model.scale.set(0.02, 0.02, 0.02);
        model.visible = false;
        scene.add(model);
      }, undefined, function(error) {
      console.error('An error happened with the GLTF loader:', error);
      });

      const aspect = window.innerWidth / window.innerHeight;
      camera = new THREE.OrthographicCamera(-aspect, aspect, 1, -1, 0.1, 20000);
      renderer = new THREE.WebGLRenderer({antialias: true,alpha:true });
      renderer.setPixelRatio( window.devicePixelRatio );
      camera.aspect = window.innerWidth / window.innerHeight;
      renderer.setSize(window.innerWidth, window.innerHeight );
      camera.updateProjectionMatrix();
      document.body.appendChild( renderer.domElement );	
      renderer.xr.enabled = true;

      function init() {
          window.addEventListener( 'resize', onWindowResize, false );
      }

      function getXRSessionInit( mode, options) {
        if ( options && options.referenceSpaceType ) {
            renderer.xr.setReferenceSpaceType( options.referenceSpaceType );
        }
        let space = (options || {}).referenceSpaceType || 'local-floor';
        let sessionInit = (options && options.sessionInit) || {};
    
        // Nothing to do for default features.
        if ( space == 'viewer' )
            return sessionInit;
        if ( space == 'local' && mode.startsWith('immersive' ) )
            return sessionInit;
    
        // If the user already specified the space as an optional or required feature, don't do anything.
        if ( sessionInit.optionalFeatures && sessionInit.optionalFeatures.includes(space) )
            return sessionInit;
        if ( sessionInit.requiredFeatures && sessionInit.requiredFeatures.includes(space) )
            return sessionInit;
    
        let newInit = Object.assign( {}, sessionInit );
        newInit.requiredFeatures = [ space ];
        if ( sessionInit.requiredFeatures ) {
            newInit.requiredFeatures = newInit.requiredFeatures.concat( sessionInit.requiredFeatures );
        }
        return newInit;
      }

      function AR(){
          let currentSession = null;
          function onSessionStarted( session ) {
              session.addEventListener( 'end', onSessionEnded );
              renderer.xr.setSession( session );
              gl = renderer.getContext()
              currentSession = session;
              session.requestReferenceSpace('local').then((refSpace) => {
              xrRefSpace = refSpace;
              session.requestAnimationFrame(onXRFrame);
              });
          }
          function onSessionEnded( /*event*/ ) {
              currentSession.removeEventListener( 'end', onSessionEnded );
              renderer.xr.setSession( null );
              currentSession = null;
          }
          if ( currentSession === null ) {
              
              let options = {
                  requiredFeatures: ['image-tracking'],
                  trackedImages: [
                  {
                      image: imgBitmap,
                      widthInMeters: 0.2
                  }
                  ]
              };
              let sessionInit = getXRSessionInit( 'immersive-ar', {
                  mode: 'immersive-ar',
                  referenceSpaceType: 'local', // 'local-floor'
                  sessionInit: options
              });
              navigator.xr.requestSession( 'immersive-ar', sessionInit ).then( onSessionStarted );
          } else {
              currentSession.end();
          }
          renderer.xr.addEventListener('sessionstart',
              function(ev) {
                  console.log('sessionstart', ev);
                  document.body.style.backgroundColor = 'rgba(0, 0, 0, 0)';
                  renderer.domElement.style.display = 'none';
              });
          renderer.xr.addEventListener('sessionend',
              function(ev) {
                  console.log('sessionend', ev);
                  document.body.style.backgroundColor = '';
                  renderer.domElement.style.display = '';
              });
      }

      function controlPosition(handX, handY) {
          let xOffset = (0.5 - handX) * 2 * lateralMovementFactor;
          let yOffset = (0.5 - handY) * 2 * verticalMovementFactor;
          model.position.x += xOffset;
          model.position.y += yOffset;
      }

      function updateShootingDirection() {
          camera.getWorldDirection(shootDirection);
          shootDirection.normalize();
      }

    function onXRFrame(t, frame) {
      const session = frame.session;
      session.requestAnimationFrame(onXRFrame);
      const baseLayer = session.renderState.baseLayer;
      const pose = frame.getViewerPose(xrRefSpace);

      render()
      if (pose) {
        for (const view of pose.views) {
          const viewport = baseLayer.getViewport(view);
          gl.viewport(viewport.x, viewport.y, viewport.width, viewport.height);
          const results = frame.getImageTrackingResults();
          for (const result of results) {
            const imageIndex = result.index;

            const pose1 = frame.getPose(result.imageSpace, xrRefSpace);
            pos = pose1.transform.position
            quat = pose1.transform.orientation
          }
        }
      }

      updateShootingDirection();
            
      // ef módel er til og því hefur ekki verið skotið þá fer það á staðsetningu myndar
      if(model && !isShot && pos != undefined){
        // birtist þegar hnefi er krepptur
        if(gestureOutput.innerText == "Closed_Fist"){
          model.visible = true;
        }
        model.position.copy( pos.toJSON())
        model.quaternion.copy(quat.toJSON())

        // ef lófinn er opnaður þá skýst model
        if (model && gestureOutput.innerText == "Open_Palm") {
          isShot = true;
        }
      }

      if(isShot) {
        // velocity er ákveðinn hraði í ákveðna átt
        // áttin er sú átt sem myndavélinn snýr að
        model.position.add(velocity);
        model.rotation.x += 0.5;
        model.rotation.y += 0.5;

        if (results.gestures.length > 0) {
            let handX = parseFloat(results.landmarks[0][0].x.toFixed(2));
            let handY = parseFloat(results.landmarks[0][0].y.toFixed(2));
            controlPosition(handX, handY);
        }

        if(model.position.z >= 40 || model.position.z <= -40) {
            isShot = false;
            model.visible = false;
        }
      }

      renderer.render(scene, camera);

      } 

    init();

    function onWindowResize() {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize( window.innerWidth, window.innerHeight );
    }

    render();

    function render() {
        renderer.render( scene, camera );
    }

  </script>
</body>
</html>

