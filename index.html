
<html lang='en'>
<head>
    <meta charset='UTF-8'>
    <title>Mediapipe Hand Gesture Recognizer</title>
</head>
<script src="https://unpkg.com/three@0.126.0/build/three.js"></script>
<script src="https://unpkg.com/three@0.126.0/examples/js/loaders/GLTFLoader.js"></script>
<body>
    <div>
      <button id='webcamButton'>ENABLE WEBCAM</button>
      <div style='position: relative;'>
          <video id='webcam' autoplay style='transform: scaleX(-1);'></video>
          <canvas id='output_canvas' style='position: absolute; left: 0px; top: 0px; transform: scaleX(-1);'></canvas>
          <h1 id='gesture_output'></h1>
          <h2>X <span id='x_output'></span></h2>
          <h2>Y <span id='y_output'></span></h2>
      </div>
  </div>

    <script type='module'>
        import { GestureRecognizer, FilesetResolver, DrawingUtils } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3';
        let gestureRecognizer;
        let runningMode = 'IMAGE';
        let enableWebcamButton;
        let webcamRunning = false;
        const videoWidth = window.screen.width;
        const videoHeight = window.screen.height;

        const createGestureRecognizer = async () => {
         
            const vision = await FilesetResolver.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm');
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task',
                    delegate: 'GPU'
                },
                runningMode: runningMode,
            });
        };
        createGestureRecognizer();

        const video = document.getElementById('webcam');
        const canvasElement = document.getElementById('output_canvas');
        const gestureOutput = document.getElementById('gesture_output');
        const xOutput = document.getElementById('x_output');
        const yOutput = document.getElementById('y_output');
        let lastVideoTime = -1;
        let results = undefined;
        let model;

        enableWebcamButton = document.getElementById('webcamButton');

        enableWebcamButton.addEventListener('click', () => {
          enableCam();
          activateXR();
        });

        function enableCam(event) {
          if (!gestureRecognizer) {
              alert('Please wait for gestureRecognizer to load');
              return;
          }
          if (webcamRunning === true) {
              webcamRunning = false;
              return;
          }
          webcamRunning = true;

          const constraints = {
              video: {
                  facingMode: "user",
              }
          };

          navigator.mediaDevices.getUserMedia(constraints)
          .then(function (stream) {
              video.srcObject = stream;
              video.addEventListener('loadeddata', predictWebcam);
          })
          .catch(function (err) {
              console.log("An error occurred: " + err);
              alert("An error occurred: " + err);
          });
      }

        async function predictWebcam() {
            const webcamElement = document.getElementById('webcam');
            if (runningMode === 'IMAGE') {
                runningMode = 'VIDEO';
                await gestureRecognizer.setOptions({ runningMode: 'VIDEO' });
            }

            let nowInMs = Date.now();

            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                results = gestureRecognizer.recognizeForVideo(video, nowInMs);
            }

            if (results.gestures.length > 0) {
                gestureOutput.style.display = 'block';
                gestureOutput.style.width = videoWidth;
                gestureOutput.innerText = results.gestures[0][0].categoryName;
                
                parseFloat(xOutput.innerText = results.landmarks[0][0].x.toFixed(2));
                parseFloat(yOutput.innerText = results.landmarks[0][0].y.toFixed(2));
                console.log(gestureOutput.innerText)
            }
            else {
                gestureOutput.style.display = 'none';
            }

            if (webcamRunning === true) {
                window.requestAnimationFrame(predictWebcam);
            }
        }

        async function activateXR() {
          const canvas = canvasElement;
          const gl = canvas.getContext("webgl", {xrCompatible: true});;

          const scene = new THREE.Scene();

          const renderer = new THREE.WebGLRenderer({
            alpha: true,
            preserveDrawingBuffer: true,
            canvas: canvas,
            context: gl
          });
          renderer.autoClear = false;

          const camera = new THREE.PerspectiveCamera();
          camera.matrixAutoUpdate = false;

          const loader = new THREE.GLTFLoader();

          loader.load("https://bjornthor21.github.io/verk5-vidmot/fireball.glb", function(gltf) {
            model = gltf.scene;
            model.scale.set(0.05, 0.05, 0.05); // Adjust these values as needed
            model.visible = false;
            scene.add(model);
          }, undefined, function(error) {
          console.error('An error happened with the GLTF loader:', error);
          });

          const session = await navigator.xr.requestSession("immersive-ar");
          session.updateRenderState({
            baseLayer: new XRWebGLLayer(session, gl)
          });

        const referenceSpace = await session.requestReferenceSpace('local');

        function positionModel(camera, model, distance) {
          let direction = new THREE.Vector3();
          camera.getWorldDirection(direction);
          let newPosition = new THREE.Vector3();
          newPosition.copy(camera.position);
          newPosition.add(direction.multiplyScalar(distance));
          model.position.copy(newPosition);
        }

        positionModel(camera, model, -0.05);


        let modelScaleFactorX = 1.0; // This could be the width of your scene
        let modelScaleFactorY = 1.0; // This could be the height of your scene

        let minX = -1.0; // minimum x-coordinate
        let maxX = 1.0;  // maximum x-coordinate
        let minY = -1.0; // minimum y-coordinate
        let maxY = 1.0;  // maximum y-coordinate
        
        function controlPosition(handX, handY) {
          // Normalize and map the coordinates
          let normalizedX = (handX - 0.5) * 2;
          let normalizedY = (handY - 0.5) * 2;

          // Adjust for any inversion in the y-axis
          normalizedY = -normalizedY;

          // Apply scaling factors (tune these values)
          let modelX = normalizedX * modelScaleFactorX * -1;
          let modelY = normalizedY * modelScaleFactorY * -1;

          // Clamp the values
          modelX = Math.max(minX, Math.min(maxX, modelX));
          modelY = Math.max(minY, Math.min(maxY, modelY));

          // Debugging: Log the values to see the calculations
          console.log(`HandX: ${handX}, HandY: ${handY}`);
          console.log(`ModelX: ${modelX}, ModelY: ${modelY}`);

          // Update the model position
          model.position.x = modelX;
          model.position.y = modelY;
        }

        let shootDirection = new THREE.Vector3();
        camera.getWorldDirection(shootDirection);
        let shootSpeed = 0.05;
        let velocity = shootDirection.multiplyScalar(shootSpeed);

          const onXRFrame = (time, frame) => {
            session.requestAnimationFrame(onXRFrame);
            gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);

            const pose = frame.getViewerPose(referenceSpace);
            if (pose) {
              const view = pose.views[0];
              const viewport = session.renderState.baseLayer.getViewport(view);
              renderer.setSize(viewport.width, viewport.height);

              camera.matrix.fromArray(view.transform.matrix);
              camera.projectionMatrix.fromArray(view.projectionMatrix);
              camera.updateMatrixWorld(true);

              renderer.render(scene, camera);
            }

            camera.getWorldDirection(shootDirection);

            if (model && gestureOutput.innerText == "Open_Palm") {
              model.visible = true;
              model.position.add(velocity);
              model.rotation.x += 0.5;
              model.rotation.y += 0.5;
              if (results.gestures.length > 0) {
                let handX = parseFloat(results.landmarks[0][0].x.toFixed(2));
                let handY = parseFloat(results.landmarks[0][0].y.toFixed(2));

                  controlPosition(handX, handY);
              }
              console.log(shootDirection);
              console.log(model.position);
            }

            if (model && gestureOutput.innerText == "Closed_Fist") {
                positionModel(camera, model, -0.05);
                model.visible = false;
            }
        };

          session.requestAnimationFrame(onXRFrame);
        }
    </script>
</body>
</html>