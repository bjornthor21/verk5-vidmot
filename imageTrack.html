<!DOCTYPE html lang="en">
<html lang="en">
<head>
    <title>AR</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
</head>

<body>
    
  <img id='bitmap' src="/verk5-vidmot/firecard.png" style="display: none;">
    <script src="https://unpkg.com/three@0.126.0/build/three.js"></script>
    <script src="https://unpkg.com/three@0.126.0/examples/js/loaders/GLTFLoader.js"></script>
    <div>
        <button id='webcamButton'>ENABLE WEBCAM</button>
        <div style='position: relative;'>
            <video id='webcam' autoplay style='transform: scaleX(-1);'></video>
            <canvas id='output_canvas' style='position: absolute; left: 0px; top: 0px; transform: scaleX(-1);'></canvas>
            <h1 id='gesture_output'></h1>
            <h2>X <span id='x_output'></span></h2>
            <h2>Y <span id='y_output'></span></h2>
        </div>
    </div>
  <script type="module">

import { GestureRecognizer, FilesetResolver, DrawingUtils } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3';
        let gestureRecognizer;
        let runningMode = 'IMAGE';
        let enableWebcamButton;
        let webcamRunning = false;
        const videoWidth = window.screen.width;
        const videoHeight = window.screen.height;

        const createGestureRecognizer = async () => {
         
            const vision = await FilesetResolver.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm');
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task',
                    delegate: 'GPU'
                },
                runningMode: runningMode,
            });
        };
        createGestureRecognizer();

        const video = document.getElementById('webcam');
        const canvasElement = document.getElementById('output_canvas');
        const gestureOutput = document.getElementById('gesture_output');
        const xOutput = document.getElementById('x_output');
        const yOutput = document.getElementById('y_output');
        let lastVideoTime = -1;
        let results = undefined;
        let model;
        let isShot = false;

        enableWebcamButton = document.getElementById('webcamButton');

        enableWebcamButton.addEventListener('click', () => {
          enableCam();
          AR();
        });

        function enableCam(event) {
          if (!gestureRecognizer) {
              alert('Please wait for gestureRecognizer to load');
              return;
          }
          if (webcamRunning === true) {
              webcamRunning = false;
              return;
          }
          webcamRunning = true;

          const constraints = {
              video: {
                  facingMode: "user",
              }
          };

          navigator.mediaDevices.getUserMedia(constraints)
          .then(function (stream) {
              video.srcObject = stream;
              video.addEventListener('loadeddata', predictWebcam);
          })
          .catch(function (err) {
              console.log("An error occurred: " + err);
              alert("An error occurred: " + err);
          });
      }

        async function predictWebcam() {
            const webcamElement = document.getElementById('webcam');
            if (runningMode === 'IMAGE') {
                runningMode = 'VIDEO';
                await gestureRecognizer.setOptions({ runningMode: 'VIDEO' });
            }

            let nowInMs = Date.now();

            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                results = gestureRecognizer.recognizeForVideo(video, nowInMs);
            }

            if (results.gestures.length > 0) {
                gestureOutput.style.display = 'block';
                gestureOutput.style.width = videoWidth;
                gestureOutput.innerText = results.gestures[0][0].categoryName;
                
                parseFloat(xOutput.innerText = results.landmarks[0][0].x.toFixed(2));
                parseFloat(yOutput.innerText = results.landmarks[0][0].y.toFixed(2));
                console.log(gestureOutput.innerText)
            }
            else {
                gestureOutput.style.display = 'none';
            }

            if (webcamRunning === true) {
                window.requestAnimationFrame(predictWebcam);
            }
        }
    
    const img  = document.getElementById('bitmap');
    let imgBitmap = null
    createImageBitmap(img).then(x=>{imgBitmap = x});
    let clock = new THREE.Clock()

    // standard webxr scene

    function xwwwform(jsonObject){
        return Object.keys(jsonObject).map(key => encodeURIComponent(key) + '=' + encodeURIComponent(jsonObject[key])).join('&');
    }

    let camera, scene, renderer, xrRefSpace, gl;

    scene = new THREE.Scene();

    const loader = new THREE.GLTFLoader();

    loader.load("https://bjornthor21.github.io/verk5-vidmot/fireball.glb", function(gltf) {
        model = gltf.scene;
        model.scale.set(0.05, 0.05, 0.05);
        model.visible = true;
        scene.add(model);
    }, undefined, function(error) {
    console.error('An error happened with the GLTF loader:', error);
    });

    const geometry = new THREE.BoxGeometry( 0.1, 0.1, 0.1 );
    const material = new THREE.MeshStandardMaterial( {color: 0xcc6600} );
    let earthCube = new THREE.Mesh( geometry, material );
    scene.add( earthCube );

    let ambient = new THREE.AmbientLight( 0x222222 );
    scene.add( ambient );
    let directionalLight = new THREE.DirectionalLight( 0xdddddd, 1.5 );
    directionalLight.position.set( 0.9, 1, 0.6 ).normalize();
    scene.add( directionalLight );
    let directionalLight2 = new THREE.DirectionalLight( 0xdddddd, 1 );
    directionalLight2.position.set( -0.9, -1, -0.4 ).normalize();
    scene.add( directionalLight2 );

    camera = new THREE.PerspectiveCamera( 80, window.innerWidth / window.innerHeight, 0.1, 20000 );
    renderer = new THREE.WebGLRenderer({antialias: true,alpha:true });
    renderer.setPixelRatio( window.devicePixelRatio );
    camera.aspect = window.innerWidth / window.innerHeight;
    renderer.setSize(window.innerWidth, window.innerHeight );
    camera.updateProjectionMatrix();
    document.body.appendChild( renderer.domElement );	
    renderer.xr.enabled = true;

    function init() {
        window.addEventListener( 'resize', onWindowResize, false );
    }

    function getXRSessionInit( mode, options) {
        if ( options && options.referenceSpaceType ) {
            renderer.xr.setReferenceSpaceType( options.referenceSpaceType );
        }
        let space = (options || {}).referenceSpaceType || 'local-floor';
        let sessionInit = (options && options.sessionInit) || {};
    
        // Nothing to do for default features.
        if ( space == 'viewer' )
            return sessionInit;
        if ( space == 'local' && mode.startsWith('immersive' ) )
            return sessionInit;
    
        // If the user already specified the space as an optional or required feature, don't do anything.
        if ( sessionInit.optionalFeatures && sessionInit.optionalFeatures.includes(space) )
            return sessionInit;
        if ( sessionInit.requiredFeatures && sessionInit.requiredFeatures.includes(space) )
            return sessionInit;
    
        let newInit = Object.assign( {}, sessionInit );
        newInit.requiredFeatures = [ space ];
        if ( sessionInit.requiredFeatures ) {
            newInit.requiredFeatures = newInit.requiredFeatures.concat( sessionInit.requiredFeatures );
        }
        return newInit;
    }

    function AR(){
        let currentSession = null;
        function onSessionStarted( session ) {
            session.addEventListener( 'end', onSessionEnded );
            renderer.xr.setSession( session );
            gl = renderer.getContext()
            currentSession = session;
            session.requestReferenceSpace('local').then((refSpace) => {
            xrRefSpace = refSpace;
            session.requestAnimationFrame(onXRFrame);
            });
        }
        function onSessionEnded( /*event*/ ) {
            currentSession.removeEventListener( 'end', onSessionEnded );
            renderer.xr.setSession( null );
            currentSession = null;
        }
        if ( currentSession === null ) {
            
            let options = {
                requiredFeatures: ['image-tracking'],
                trackedImages: [
                {
                    image: imgBitmap,
                    widthInMeters: 0.2
                }
                ]
            };
            let sessionInit = getXRSessionInit( 'immersive-ar', {
                mode: 'immersive-ar',
                referenceSpaceType: 'local', // 'local-floor'
                sessionInit: options
            });
            navigator.xr.requestSession( 'immersive-ar', sessionInit ).then( onSessionStarted );
        } else {
            currentSession.end();
        }
        renderer.xr.addEventListener('sessionstart',
            function(ev) {
                console.log('sessionstart', ev);
                document.body.style.backgroundColor = 'rgba(0, 0, 0, 0)';
                renderer.domElement.style.display = 'none';
            });
        renderer.xr.addEventListener('sessionend',
            function(ev) {
                console.log('sessionend', ev);
                document.body.style.backgroundColor = '';
                renderer.domElement.style.display = '';
            });
    }

    function onXRFrame(t, frame) {
        const session = frame.session;
        session.requestAnimationFrame(onXRFrame);
        const baseLayer = session.renderState.baseLayer;
        const pose = frame.getViewerPose(xrRefSpace);
        render()
        if (pose) {
            for (const view of pose.views) {
                const viewport = baseLayer.getViewport(view);
                gl.viewport(viewport.x, viewport.y,
                            viewport.width, viewport.height);
                const results = frame.getImageTrackingResults();
                for (const result of results) {
                // The result's index is the image's position in the trackedImages array specified at session creation
                const imageIndex = result.index;
                
                // Get the pose of the image relative to a reference space.
                const pose1 = frame.getPose(result.imageSpace, xrRefSpace);
                let pos = pose1.transform.position
                let quat = pose1.transform.orientation

                model.position.copy( pos.toJSON())
                model.quaternion.copy(quat.toJSON())
                const state = result.trackingState;
                
                if (state == "tracked") {
                    // HighlightImage(imageIndex, pose1);
                } else if (state == "emulated") {
                    // FadeImage(imageIndex, pose1);
                }


                }
            }
            
        } // if pose

    }
    init()
    function onWindowResize() {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize( window.innerWidth, window.innerHeight );
    }
    render()
    function render() {
        renderer.render( scene, camera );
    }

  </script>
</body>
</html>

